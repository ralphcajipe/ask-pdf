{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application: Ask Questions from a PDF Document using Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is a generative AI framework that combines pre-trained large language models (LLMs) with external data sources. RAG improves the output of LLMs by using fresh data from authoritative knowledge bases and enterprise systems to generate more reliable responses.\n",
    "\n",
    "For example, this project is about using RAG to ask questions from a PDF document. The RAG system uses its large language model to understand the question, then it retrieves relevant information from the PDF document, and finally generates a response. This way, we can extract precise information from a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Ollama\n",
    "\n",
    "I used [Ollama](https://ollama.com) because it's the easiest way to get up and running with large language models, locally on my computer.\n",
    "\n",
    "In this case, I used [TinyLlama](https://arxiv.org/pdf/2401.02385.pdf) model by StatNLP Research Group and \n",
    "Singapore University of Technology and Design.\n",
    "\n",
    "On your terminal, run:\n",
    "\n",
    "```bash\n",
    "ollama run tinyllama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Environment Variables and Setting Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# If you want to use the OpenAI API, you need to set the OPENAI_API_KEY environment variable\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "MODEL = \"tinyllama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Embeddings and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning (ML) is a field of artificial intelligence that enables machines to learn from experience and improve their performance over time without being programmed specifically for those tasks. In simple terms, ML helps machines to \"learn\" from data or experiences to make decisions and perform tasks better than they would be able to do on their own.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "model.invoke(\"what is machine learning in a few words?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning (ML) is a field of artificial intelligence that allows computers to learn and improve their own behavior based on data inputted by humans or other machines. In simpler terms, ML is a type of AI algorithm that enables machines to think like humans and improve on their performance based on the patterns they encounter. It involves using algorithms to process massive amounts of data and make predictions or decisions based on it, which can be beneficial for various applications in various industries.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "chain.invoke(\"what is machine learning in a few words?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3/10/24, 8:32 PM Gemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/ 1/5DEVELOPERS\\nGemma: Introducing new state-\\nof-the-art open models\\nGemma is built for responsible AI development from the same research and\\ntechnology used to create Gemini models.\\nFeb 21, 2024·3 min read\\nJJeanine Banks\\nVP & GM, Developer X and\\nDevRelTTris Warkentin\\nDirector, Google DeepMindShare\\nListen to article\\n7 minutes\\nThe Keyword', metadata={'source': 'gemma-model.pdf', 'page': 0}),\n",
       " Document(page_content='3/10/24, 8:32 PM Gemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/ 2/5At Google, we believe in making AI helpful for everyone. We have a long history of contributing innovations\\nto the open community, such as with Transformers, TensorFlow, BERT, T5, JAX, AlphaFold, and AlphaCode.\\nToday, we’re excited to introduce a new generation of open models from Google to assist developers and\\nresearchers in building AI responsibly.\\nGemm a open models\\nGemma is a family of lightweight, state-of-the-art open models built from the same research and\\ntechnology used to create the Gemini models. Developed by Google DeepMind and other teams across\\nGoogle, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning “precious stone.”\\nAccompanying our model weights, we’re also releasing tools to support developer innovation, foster\\ncollaboration, and guide responsible use of Gemma models.\\nGemma is available worldwide, starting today. Here are the key details to know:\\nWe’re releasing model weights in two sizes: Gemma 2B and Gemma 7B. Each size is released with pre-\\ntrained and instruction-tuned variants.\\nA new Responsible Generative AI Toolkit provides guidance and essential tools for creating safer AI\\napplications with Gemma.\\nWe’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks:\\nJAX, PyTorch, and TensorFlow through native Keras 3.0.\\nReady-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging\\nFace, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma.\\nPre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud\\nwith easy deployment on Vertex AI and Google Kubernetes Engine (GKE).\\nOptimization across multiple AI hardware platforms ensures industry-leading performance, including\\nNVIDIA GPUs and Google Cloud TPUs.\\nTerms of use permit responsible commercial usage and distribution for all organizations, regardless of\\nsize.\\nState-of-the-art performance at size\\nGemma models share technical and infrastructure components with Gemini, our largest and most capable\\nAI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for', metadata={'source': 'gemma-model.pdf', 'page': 1}),\n",
       " Document(page_content='3/10/24, 8:32 PM Gemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/ 3/5their sizes compared to other open models. And Gemma models are capable of running directly on a\\ndeveloper laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key\\nbenchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical\\nreport for details on performance, dataset composition, and modeling methodologies.\\nResponsible by design\\nGemma is designed with our AI Principles at the forefront. As part of making Gemma pre-trained models\\nsafe and reliable, we used automated techniques to filter out certain personal information and other\\nsensitive data from training sets. Additionally, we used extensive fine-tuning and reinforcement learning\\nfrom human feedback (RLHF) to align our instruction-tuned models with responsible behaviors. To\\nunderstand and reduce the risk profile for Gemma models, we conducted robust evaluations including\\nmanual red-teaming, automated adversarial testing, and assessments of model capabilities for dangerous\\nactivities. These evaluations are outlined in our Model Card.\\nWe’re also releasing a new Responsible Generative AI Toolkit together with Gemma to help developers and\\nresearchers prioritize building safe and responsible AI applications. The toolkit includes:\\n1', metadata={'source': 'gemma-model.pdf', 'page': 2}),\n",
       " Document(page_content=\"3/10/24, 8:32 PM Gemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/ 4/5Safety classification: We provide a novel methodology for building robust safety classifiers with\\nminimal examples.\\nDebugging: A model debugging tool helps you investigate Gemma's behavior and address potential\\nissues.\\nGuidance: You can access best practices for model builders based on Google’s experience in\\ndeveloping and deploying large language models.\\nOptimized across frameworks, tools and hardware\\nYou can fine-tune Gemma models on your own data to adapt to specific application needs, such as\\nsummarization or retrieval-augmented generation (RAG). Gemma supports a wide variety of tools and\\nsystems:\\nMulti-framework tools: Bring your favorite framework, with reference implementations for inference\\nand fine-tuning across multi-framework Keras 3.0, native PyTorch, JAX, and Hugging Face\\nTransformers.\\nCross-device compatibility: Gemma models run across popular device types, including laptop,\\ndesktop, IoT, mobile and cloud, enabling broadly accessible AI capabilities.\\nCutting-edge hardware platforms: We’ve partnered with NVIDIA to optimize Gemma for NVIDIA GPUs,\\nfrom data center to the cloud to local RTX AI PCs, ensuring industry-leading performance and\\nintegration with cutting-edge technology.\\nOptimized for Google Cloud: Vertex AI provides a broad MLOps toolset with a range of tuning options\\nand one-click deployment using built-in inference optimizations. Advanced customization is available\\nwith fully-managed Vertex AI tools or with self-managed GKE, including deployment to cost-efficient\\ninfrastructure across GPU, TPU, and CPU from either platform.\\nFree credits for research and development\\nGemma is built for the open community of developers and researchers powering AI innovation. You can\\nstart working with Gemma today using free access in Kaggle, a free tier for Colab notebooks, and $300 in\\ncredits for first-time Google Cloud users. Researchers can also apply for Google Cloud credits of up to a\\ncollective $500,000 to accelerate their projects.\", metadata={'source': 'gemma-model.pdf', 'page': 3}),\n",
       " Document(page_content='3/10/24, 8:32 PM Gemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/ 5/5Ge\\x00ing started\\nYou can explore more about Gemma and access quickstart guides on ai.google.dev/gemma.\\nAs we continue to expand the Gemma model family, we look forward to introducing new variants for\\ndiverse applications. Stay tuned for events and opportunities in the coming weeks to connect, learn and\\nbuild with Gemma.\\nWe’re excited to see what you create!\\nPOSTED IN:\\nDevelopers  AI\\nMore Information\\nCollapseGoogle adheres to rigorous data filtering practices to ensure fair evaluation. Our models exclude\\nbenchmark data from training sets, ensuring the integrity of benchmark comparisons.1', metadata={'source': 'gemma-model.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"gemma-model.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, respond with \"I don't know\".\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, respond with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain the Prompt, Model, and Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can't answer for your college background due to the lack of context. However, in general, a degree in computer science or a related field would indicate that you have a thorough understanding of machine learning and web application development. If your program includes the deployment feature, it's likely that you possess relevant knowledge and skills in this area as well.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"i like to deploy machine learning models as web apps\", \n",
    "        \"question\": \"what do you think is my college background?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Use a Vector Database to Store and Retrieve the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xl38\\Desktop\\ml-projects\\ml\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What makes the Gemma model special?\n",
      "Answer: The Gemma model is unique in several ways, including its large size and ability to run directly on a developer's laptop or desktop computer without the need for specialized hardware. Additionally, Gemma surpasses other open models on key benchmarks while adhering to our responsible and safe outputs guidelines. These factors make Gemma an effective choice for developers and researchers looking to build safe and responsible AI applications.\n",
      "\n",
      "Question: Why is Gemma model a new state-of-the-art?\n",
      "Answer: Answer: Gemma model is a new and innovative open models that are designed with responsible and safe outputs, as per the AI Principles. It surpasses significantly larger models on key benchmarks while adhering to our rigorous standards for safe and responsible outputs. This makes it an excellent choice for developers and researchers in the field of AI development from a trustworthy and responsible perspective.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What makes the Gemma model special?\",\n",
    "    \"Why is Gemma model a new state-of-the-art?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Streaming Questions to the Language Model\n",
    "Basically, what stream does is make the response appear like the style of a chatbot because of a typewriter effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to the question is \"Yes, you can fine-tune Gemma models on your own data.\" This means that you can train and tune Gemma models on your specific data and use them for different applications or tasks. In other words, you can customize the training process and ensure that the model is tailored to your needs."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\": \"Can I fine-tune Gemma on my own data?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Batching Questions to the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, what batch does is that it allows you to send a batch of questions to the model. This is useful when you have a lot of questions to ask and you don't want to wait for the model to process each question one by one. This is done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Can I use TensorFlow and Keras with Gemma?\",\n",
    "    \"Is there debugging support?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Yes, TensoRFlow and Keras can be used with Gemma for training and evaluating AI models. As mentioned in the context given earlier, Gemma's state-of-the-art open models support multiple frameworks, tools, and hardware platforms, including NVIDIA GPUs, from data center to the cloud to local RTX AI PCs, ensuring industry-leading performance and cost-efficient infrastructure across GPU, TPU, and CPU. \\n\\nFurthermore, Gemma is designed with our AI Principles at the forefront, making it safe and reliable for responsible AI development while using extensive fine-tuining and reinforce learning techniques to align its instruction-tuned models with responsible behavior standards. It also conducts robust evaluation with manual red-teaming, automatied adversarial testing, and assessments of model capabilities for dangerous activities to prioritize building safe and responsible AI applications.\",\n",
       " 'Yes, the given document (i.e., [document(page_content=\"3/10/24, 8:32 PM Gemma: Google introduce...\")]) provides information on the new state-of-the-art open models introduced by Google and their performance compared to other open models. While not explicitly stated as debugging support, it is mentioned that these models are capable of running directly on a developer\\'s laptop or desktop computer while adhering to our AI Principles at the forefront and being designed with responsible behavior in mind. Furthermore, there is also a new Responsible Generative AI Toolkit available for developers and researchers who want to prioritize building safe and responsible AI applications.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain.batch([{\"question\": q} for q in questions])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
